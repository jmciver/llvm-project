// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --version 2
// RUN: %clang_cc1 -no-enable-noundef-analysis %s -cl-std=CL2.0 -ffake-address-space-map -O0 -emit-llvm -o - -triple "spir-unknown-unknown" | FileCheck %s --check-prefixes=COMMON,B32,SPIR
// RUN: %clang_cc1 -no-enable-noundef-analysis %s -cl-std=CL2.0 -ffake-address-space-map -O0 -emit-llvm -o - -triple "spir64-unknown-unknown" | FileCheck %s --check-prefixes=COMMON,B64,SPIR
// RUN: %clang_cc1 -no-enable-noundef-analysis %s -cl-std=CL2.0 -ffake-address-space-map -O1 -emit-llvm -o - -triple "spir64-unknown-unknown" | FileCheck %s --check-prefix=CHECK-LIFETIMES
// RUN: %clang_cc1 -no-enable-noundef-analysis %s -cl-std=CL3.0 -ffake-address-space-map -O0 -emit-llvm -o - -triple "spir-unknown-unknown" | FileCheck %s --check-prefixes=COMMON,B32,SPIR
// RUN: %clang_cc1 -no-enable-noundef-analysis %s -cl-std=CL3.0 -ffake-address-space-map -O0 -emit-llvm -o - -triple "spir64-unknown-unknown" | FileCheck %s --check-prefixes=COMMON,B64,SPIR
// RUN: %clang_cc1 -no-enable-noundef-analysis %s -cl-std=CL3.0 -ffake-address-space-map -O1 -emit-llvm -o - -triple "spir64-unknown-unknown" | FileCheck %s --check-prefix=CHECK-LIFETIMES
// RUN: %clang_cc1 -no-enable-noundef-analysis %s -cl-std=CL2.0 -ffake-address-space-map -O0 -emit-llvm -o - -triple "x86_64-unknown-linux-gnu" | FileCheck %s --check-prefixes=COMMON,B64,X86
// RUN: %clang_cc1 -no-enable-noundef-analysis %s -cl-std=CL3.0 -ffake-address-space-map -O0 -emit-llvm -o - -triple "x86_64-unknown-linux-gnu" | FileCheck %s --check-prefixes=COMMON,B64,X86
// RUN: %clang_cc1 -no-enable-noundef-analysis %s -cl-std=CL3.0 -ffake-address-space-map -O1 -emit-llvm -o - -triple "x86_64-unknown-linux-gnu" | FileCheck %s --check-prefix=CHECK-LIFETIMES

#pragma OPENCL EXTENSION cl_khr_subgroups : enable

typedef void (^bl_t)(local void *);
typedef struct {int a;} ndrange_t;

// For a block global variable, first emit the block literal as a global variable, then emit the block variable itself.

// For anonymous blocks without captures, emit block literals as global variable.

// Emits block literal [[BL_GLOBAL]], invoke function [[INV_G]] and global block variable @block_G
const bl_t block_G = (bl_t) ^ (local void *a) {};

// B32-LABEL: define dso_local spir_func void @callee
// B32-SAME: (i32 [[ID:%.*]], ptr addrspace(1) [[OUT:%.*]]) #[[ATTR2:[0-9]+]] {
// B32-NEXT:  entry:
// B32-NEXT:    [[ID_ADDR:%.*]] = alloca i32, align 4
// B32-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 4
// B32-NEXT:    store i32 [[ID]], ptr [[ID_ADDR]], align 4
// B32-NEXT:    store ptr addrspace(1) [[OUT]], ptr [[OUT_ADDR]], align 4
// B32-NEXT:    [[TMP0:%.*]] = load i32, ptr [[ID_ADDR]], align 4
// B32-NEXT:    [[TMP1:%.*]] = load ptr addrspace(1), ptr [[OUT_ADDR]], align 4
// B32-NEXT:    [[TMP2:%.*]] = load i32, ptr [[ID_ADDR]], align 4
// B32-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr addrspace(1) [[TMP1]], i32 [[TMP2]]
// B32-NEXT:    store i32 [[TMP0]], ptr addrspace(1) [[ARRAYIDX]], align 4
// B32-NEXT:    ret void
//
// X86-LABEL: define dso_local void @callee
// X86-SAME: (i32 [[ID:%.*]], ptr addrspace(1) [[OUT:%.*]]) #[[ATTR2:[0-9]+]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[ID_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8
// X86-NEXT:    store i32 [[ID]], ptr [[ID_ADDR]], align 4
// X86-NEXT:    store ptr addrspace(1) [[OUT]], ptr [[OUT_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load i32, ptr [[ID_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr addrspace(1), ptr [[OUT_ADDR]], align 8
// X86-NEXT:    [[TMP2:%.*]] = load i32, ptr [[ID_ADDR]], align 4
// X86-NEXT:    [[IDXPROM:%.*]] = sext i32 [[TMP2]] to i64
// X86-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr addrspace(1) [[TMP1]], i64 [[IDXPROM]]
// X86-NEXT:    store i32 [[TMP0]], ptr addrspace(1) [[ARRAYIDX]], align 4
// X86-NEXT:    ret void
//
void callee(int id, __global int *out) {
  out[id] = id;
}

// B32-LABEL: define dso_local spir_kernel void @device_side_enqueue
// B32-SAME: (ptr addrspace(1) align 4 [[A:%.*]], ptr addrspace(1) align 4 [[B:%.*]], i32 [[I:%.*]]) #[[ATTR3:[0-9]+]] !kernel_arg_addr_space !3 !kernel_arg_access_qual !4 !kernel_arg_type !5 !kernel_arg_base_type !5 !kernel_arg_type_qual !6 {
// B32-NEXT:  entry:
// B32-NEXT:    [[A_ADDR:%.*]] = alloca ptr addrspace(1), align 4
// B32-NEXT:    [[B_ADDR:%.*]] = alloca ptr addrspace(1), align 4
// B32-NEXT:    [[I_ADDR:%.*]] = alloca i32, align 4
// B32-NEXT:    [[DEFAULT_QUEUE:%.*]] = alloca target("spirv.Queue"), align 4
// B32-NEXT:    [[FLAGS:%.*]] = alloca i32, align 4
// B32-NEXT:    [[NDRANGE:%.*]] = alloca [[STRUCT_NDRANGE_T:%.*]], align 4
// B32-NEXT:    [[CLK_EVENT:%.*]] = alloca target("spirv.DeviceEvent"), align 4
// B32-NEXT:    [[EVENT_WAIT_LIST:%.*]] = alloca target("spirv.DeviceEvent"), align 4
// B32-NEXT:    [[EVENT_WAIT_LIST2:%.*]] = alloca [1 x target("spirv.DeviceEvent")], align 4
// B32-NEXT:    [[TMP:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// B32-NEXT:    [[BLOCK:%.*]] = alloca <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32, ptr addrspace(1) }>, align 4
// B32-NEXT:    [[TMP3:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// B32-NEXT:    [[BLOCK4:%.*]] = alloca <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32, ptr addrspace(1) }>, align 4
// B32-NEXT:    [[TMP11:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// B32-NEXT:    [[TMP12:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// B32-NEXT:    [[BLOCK_SIZES:%.*]] = alloca [1 x i32], align 4
// B32-NEXT:    [[C:%.*]] = alloca i8, align 1
// B32-NEXT:    [[TMP13:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// B32-NEXT:    [[BLOCK_SIZES14:%.*]] = alloca [1 x i32], align 4
// B32-NEXT:    [[TMP15:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// B32-NEXT:    [[BLOCK_SIZES16:%.*]] = alloca [1 x i32], align 4
// B32-NEXT:    [[TMP17:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// B32-NEXT:    [[BLOCK_SIZES19:%.*]] = alloca [1 x i32], align 4
// B32-NEXT:    [[L:%.*]] = alloca i64, align 8
// B32-NEXT:    [[TMP20:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// B32-NEXT:    [[BLOCK_SIZES21:%.*]] = alloca [1 x i32], align 4
// B32-NEXT:    [[TMP22:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// B32-NEXT:    [[BLOCK_SIZES23:%.*]] = alloca [3 x i32], align 4
// B32-NEXT:    [[TMP24:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// B32-NEXT:    [[BLOCK_SIZES25:%.*]] = alloca [1 x i32], align 4
// B32-NEXT:    [[BLOCK_A:%.*]] = alloca ptr addrspace(4), align 4
// B32-NEXT:    [[BLOCK_B:%.*]] = alloca ptr addrspace(4), align 4
// B32-NEXT:    [[TMP26:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// B32-NEXT:    [[SIZE:%.*]] = alloca i32, align 4
// B32-NEXT:    [[B1:%.*]] = alloca ptr addrspace(4), align 4
// B32-NEXT:    [[B2:%.*]] = alloca ptr addrspace(4), align 4
// B32-NEXT:    [[BLOCK_C:%.*]] = alloca ptr addrspace(4), align 4
// B32-NEXT:    [[BLOCK27:%.*]] = alloca <{ i32, i32, ptr addrspace(4), i32, ptr addrspace(1) }>, align 4
// B32-NEXT:    [[TMP33:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// B32-NEXT:    [[TMP34:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// B32-NEXT:    [[TMP35:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// B32-NEXT:    store ptr addrspace(1) [[A]], ptr [[A_ADDR]], align 4
// B32-NEXT:    store ptr addrspace(1) [[B]], ptr [[B_ADDR]], align 4
// B32-NEXT:    store i32 [[I]], ptr [[I_ADDR]], align 4
// B32-NEXT:    store i32 0, ptr [[FLAGS]], align 4
// B32-NEXT:    [[ARRAYINIT_BEGIN:%.*]] = getelementptr inbounds [1 x target("spirv.DeviceEvent")], ptr [[EVENT_WAIT_LIST2]], i32 0, i32 0
// B32-NEXT:    [[TMP0:%.*]] = load target("spirv.DeviceEvent"), ptr [[CLK_EVENT]], align 4
// B32-NEXT:    store target("spirv.DeviceEvent") [[TMP0]], ptr [[ARRAYINIT_BEGIN]], align 4
// B32-NEXT:    [[TMP1:%.*]] = load target("spirv.Queue"), ptr [[DEFAULT_QUEUE]], align 4
// B32-NEXT:    [[TMP2:%.*]] = load i32, ptr [[FLAGS]], align 4
// B32-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 4 [[TMP]], ptr align 4 [[NDRANGE]], i32 4, i1 false)
// B32-NEXT:    [[BLOCK_SIZE:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32, ptr addrspace(1) }>, ptr [[BLOCK]], i32 0, i32 0
// B32-NEXT:    store i32 24, ptr [[BLOCK_SIZE]], align 4
// B32-NEXT:    [[BLOCK_ALIGN:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32, ptr addrspace(1) }>, ptr [[BLOCK]], i32 0, i32 1
// B32-NEXT:    store i32 4, ptr [[BLOCK_ALIGN]], align 4
// B32-NEXT:    [[BLOCK_INVOKE:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32, ptr addrspace(1) }>, ptr [[BLOCK]], i32 0, i32 2
// B32-NEXT:    store ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke to ptr addrspace(4)), ptr [[BLOCK_INVOKE]], align 4
// B32-NEXT:    [[BLOCK_CAPTURED:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32, ptr addrspace(1) }>, ptr [[BLOCK]], i32 0, i32 3
// B32-NEXT:    [[TMP3:%.*]] = load ptr addrspace(1), ptr [[A_ADDR]], align 4
// B32-NEXT:    store ptr addrspace(1) [[TMP3]], ptr [[BLOCK_CAPTURED]], align 4
// B32-NEXT:    [[BLOCK_CAPTURED1:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32, ptr addrspace(1) }>, ptr [[BLOCK]], i32 0, i32 4
// B32-NEXT:    [[TMP4:%.*]] = load i32, ptr [[I_ADDR]], align 4
// B32-NEXT:    store i32 [[TMP4]], ptr [[BLOCK_CAPTURED1]], align 4
// B32-NEXT:    [[BLOCK_CAPTURED2:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32, ptr addrspace(1) }>, ptr [[BLOCK]], i32 0, i32 5
// B32-NEXT:    [[TMP5:%.*]] = load ptr addrspace(1), ptr [[B_ADDR]], align 4
// B32-NEXT:    store ptr addrspace(1) [[TMP5]], ptr [[BLOCK_CAPTURED2]], align 4
// B32-NEXT:    [[TMP6:%.*]] = addrspacecast ptr [[BLOCK]] to ptr addrspace(4)
// B32-NEXT:    [[TMP7:%.*]] = call spir_func i32 @__enqueue_kernel_basic(target("spirv.Queue") [[TMP1]], i32 [[TMP2]], ptr byval([[STRUCT_NDRANGE_T]]) [[TMP]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_kernel to ptr addrspace(4)), ptr addrspace(4) [[TMP6]])
// B32-NEXT:    [[TMP8:%.*]] = load target("spirv.Queue"), ptr [[DEFAULT_QUEUE]], align 4
// B32-NEXT:    [[TMP9:%.*]] = load i32, ptr [[FLAGS]], align 4
// B32-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 4 [[TMP3]], ptr align 4 [[NDRANGE]], i32 4, i1 false)
// B32-NEXT:    [[TMP10:%.*]] = addrspacecast ptr [[EVENT_WAIT_LIST]] to ptr addrspace(4)
// B32-NEXT:    [[TMP11:%.*]] = addrspacecast ptr [[CLK_EVENT]] to ptr addrspace(4)
// B32-NEXT:    [[BLOCK_SIZE5:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32, ptr addrspace(1) }>, ptr [[BLOCK4]], i32 0, i32 0
// B32-NEXT:    store i32 24, ptr [[BLOCK_SIZE5]], align 4
// B32-NEXT:    [[BLOCK_ALIGN6:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32, ptr addrspace(1) }>, ptr [[BLOCK4]], i32 0, i32 1
// B32-NEXT:    store i32 4, ptr [[BLOCK_ALIGN6]], align 4
// B32-NEXT:    [[BLOCK_INVOKE7:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32, ptr addrspace(1) }>, ptr [[BLOCK4]], i32 0, i32 2
// B32-NEXT:    store ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_2 to ptr addrspace(4)), ptr [[BLOCK_INVOKE7]], align 4
// B32-NEXT:    [[BLOCK_CAPTURED8:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32, ptr addrspace(1) }>, ptr [[BLOCK4]], i32 0, i32 3
// B32-NEXT:    [[TMP12:%.*]] = load ptr addrspace(1), ptr [[A_ADDR]], align 4
// B32-NEXT:    store ptr addrspace(1) [[TMP12]], ptr [[BLOCK_CAPTURED8]], align 4
// B32-NEXT:    [[BLOCK_CAPTURED9:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32, ptr addrspace(1) }>, ptr [[BLOCK4]], i32 0, i32 4
// B32-NEXT:    [[TMP13:%.*]] = load i32, ptr [[I_ADDR]], align 4
// B32-NEXT:    store i32 [[TMP13]], ptr [[BLOCK_CAPTURED9]], align 4
// B32-NEXT:    [[BLOCK_CAPTURED10:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32, ptr addrspace(1) }>, ptr [[BLOCK4]], i32 0, i32 5
// B32-NEXT:    [[TMP14:%.*]] = load ptr addrspace(1), ptr [[B_ADDR]], align 4
// B32-NEXT:    store ptr addrspace(1) [[TMP14]], ptr [[BLOCK_CAPTURED10]], align 4
// B32-NEXT:    [[TMP15:%.*]] = addrspacecast ptr [[BLOCK4]] to ptr addrspace(4)
// B32-NEXT:    [[TMP16:%.*]] = call spir_func i32 @__enqueue_kernel_basic_events(target("spirv.Queue") [[TMP8]], i32 [[TMP9]], ptr [[TMP3]], i32 2, ptr addrspace(4) [[TMP10]], ptr addrspace(4) [[TMP11]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_2_kernel to ptr addrspace(4)), ptr addrspace(4) [[TMP15]])
// B32-NEXT:    [[TMP17:%.*]] = load target("spirv.Queue"), ptr [[DEFAULT_QUEUE]], align 4
// B32-NEXT:    [[TMP18:%.*]] = load i32, ptr [[FLAGS]], align 4
// B32-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 4 [[TMP11]], ptr align 4 [[NDRANGE]], i32 4, i1 false)
// B32-NEXT:    [[TMP19:%.*]] = call spir_func i32 @__enqueue_kernel_basic_events(target("spirv.Queue") [[TMP17]], i32 [[TMP18]], ptr [[TMP11]], i32 1, ptr addrspace(4) null, ptr addrspace(4) null, ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_3_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.1 to ptr addrspace(4)))
// B32-NEXT:    [[TMP20:%.*]] = load target("spirv.Queue"), ptr [[DEFAULT_QUEUE]], align 4
// B32-NEXT:    [[TMP21:%.*]] = load i32, ptr [[FLAGS]], align 4
// B32-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 4 [[TMP12]], ptr align 4 [[NDRANGE]], i32 4, i1 false)
// B32-NEXT:    [[TMP22:%.*]] = getelementptr [1 x i32], ptr [[BLOCK_SIZES]], i32 0, i32 0
// B32-NEXT:    store i32 256, ptr [[TMP22]], align 4
// B32-NEXT:    [[TMP23:%.*]] = call spir_func i32 @__enqueue_kernel_varargs(target("spirv.Queue") [[TMP20]], i32 [[TMP21]], ptr [[TMP12]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_4_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.2 to ptr addrspace(4)), i32 1, ptr [[TMP22]])
// B32-NEXT:    [[TMP24:%.*]] = load target("spirv.Queue"), ptr [[DEFAULT_QUEUE]], align 4
// B32-NEXT:    [[TMP25:%.*]] = load i32, ptr [[FLAGS]], align 4
// B32-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 4 [[TMP13]], ptr align 4 [[NDRANGE]], i32 4, i1 false)
// B32-NEXT:    [[TMP26:%.*]] = getelementptr [1 x i32], ptr [[BLOCK_SIZES14]], i32 0, i32 0
// B32-NEXT:    [[TMP27:%.*]] = load i8, ptr [[C]], align 1
// B32-NEXT:    [[TMP28:%.*]] = zext i8 [[TMP27]] to i32
// B32-NEXT:    store i32 [[TMP28]], ptr [[TMP26]], align 4
// B32-NEXT:    [[TMP29:%.*]] = call spir_func i32 @__enqueue_kernel_varargs(target("spirv.Queue") [[TMP24]], i32 [[TMP25]], ptr [[TMP13]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_5_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.3 to ptr addrspace(4)), i32 1, ptr [[TMP26]])
// B32-NEXT:    [[TMP30:%.*]] = load target("spirv.Queue"), ptr [[DEFAULT_QUEUE]], align 4
// B32-NEXT:    [[TMP31:%.*]] = load i32, ptr [[FLAGS]], align 4
// B32-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 4 [[TMP15]], ptr align 4 [[NDRANGE]], i32 4, i1 false)
// B32-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [1 x target("spirv.DeviceEvent")], ptr [[EVENT_WAIT_LIST2]], i32 0, i32 0
// B32-NEXT:    [[TMP32:%.*]] = addrspacecast ptr [[ARRAYDECAY]] to ptr addrspace(4)
// B32-NEXT:    [[TMP33:%.*]] = addrspacecast ptr [[CLK_EVENT]] to ptr addrspace(4)
// B32-NEXT:    [[TMP34:%.*]] = getelementptr [1 x i32], ptr [[BLOCK_SIZES16]], i32 0, i32 0
// B32-NEXT:    store i32 256, ptr [[TMP34]], align 4
// B32-NEXT:    [[TMP35:%.*]] = call spir_func i32 @__enqueue_kernel_events_varargs(target("spirv.Queue") [[TMP30]], i32 [[TMP31]], ptr [[TMP15]], i32 2, ptr addrspace(4) [[TMP32]], ptr addrspace(4) [[TMP33]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_6_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.4 to ptr addrspace(4)), i32 1, ptr [[TMP34]])
// B32-NEXT:    [[TMP36:%.*]] = load target("spirv.Queue"), ptr [[DEFAULT_QUEUE]], align 4
// B32-NEXT:    [[TMP37:%.*]] = load i32, ptr [[FLAGS]], align 4
// B32-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 4 [[TMP17]], ptr align 4 [[NDRANGE]], i32 4, i1 false)
// B32-NEXT:    [[ARRAYDECAY18:%.*]] = getelementptr inbounds [1 x target("spirv.DeviceEvent")], ptr [[EVENT_WAIT_LIST2]], i32 0, i32 0
// B32-NEXT:    [[TMP38:%.*]] = addrspacecast ptr [[ARRAYDECAY18]] to ptr addrspace(4)
// B32-NEXT:    [[TMP39:%.*]] = addrspacecast ptr [[CLK_EVENT]] to ptr addrspace(4)
// B32-NEXT:    [[TMP40:%.*]] = getelementptr [1 x i32], ptr [[BLOCK_SIZES19]], i32 0, i32 0
// B32-NEXT:    [[TMP41:%.*]] = load i8, ptr [[C]], align 1
// B32-NEXT:    [[TMP42:%.*]] = zext i8 [[TMP41]] to i32
// B32-NEXT:    store i32 [[TMP42]], ptr [[TMP40]], align 4
// B32-NEXT:    [[TMP43:%.*]] = call spir_func i32 @__enqueue_kernel_events_varargs(target("spirv.Queue") [[TMP36]], i32 [[TMP37]], ptr [[TMP17]], i32 2, ptr addrspace(4) [[TMP38]], ptr addrspace(4) [[TMP39]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_7_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.5 to ptr addrspace(4)), i32 1, ptr [[TMP40]])
// B32-NEXT:    [[TMP44:%.*]] = load target("spirv.Queue"), ptr [[DEFAULT_QUEUE]], align 4
// B32-NEXT:    [[TMP45:%.*]] = load i32, ptr [[FLAGS]], align 4
// B32-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 4 [[TMP20]], ptr align 4 [[NDRANGE]], i32 4, i1 false)
// B32-NEXT:    [[TMP46:%.*]] = getelementptr [1 x i32], ptr [[BLOCK_SIZES21]], i32 0, i32 0
// B32-NEXT:    [[TMP47:%.*]] = load i64, ptr [[L]], align 8
// B32-NEXT:    [[TMP48:%.*]] = trunc i64 [[TMP47]] to i32
// B32-NEXT:    store i32 [[TMP48]], ptr [[TMP46]], align 4
// B32-NEXT:    [[TMP49:%.*]] = call spir_func i32 @__enqueue_kernel_varargs(target("spirv.Queue") [[TMP44]], i32 [[TMP45]], ptr [[TMP20]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_8_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.6 to ptr addrspace(4)), i32 1, ptr [[TMP46]])
// B32-NEXT:    [[TMP50:%.*]] = load target("spirv.Queue"), ptr [[DEFAULT_QUEUE]], align 4
// B32-NEXT:    [[TMP51:%.*]] = load i32, ptr [[FLAGS]], align 4
// B32-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 4 [[TMP22]], ptr align 4 [[NDRANGE]], i32 4, i1 false)
// B32-NEXT:    [[TMP52:%.*]] = getelementptr [3 x i32], ptr [[BLOCK_SIZES23]], i32 0, i32 0
// B32-NEXT:    store i32 1, ptr [[TMP52]], align 4
// B32-NEXT:    [[TMP53:%.*]] = getelementptr [3 x i32], ptr [[BLOCK_SIZES23]], i32 0, i32 1
// B32-NEXT:    store i32 2, ptr [[TMP53]], align 4
// B32-NEXT:    [[TMP54:%.*]] = getelementptr [3 x i32], ptr [[BLOCK_SIZES23]], i32 0, i32 2
// B32-NEXT:    store i32 4, ptr [[TMP54]], align 4
// B32-NEXT:    [[TMP55:%.*]] = call spir_func i32 @__enqueue_kernel_varargs(target("spirv.Queue") [[TMP50]], i32 [[TMP51]], ptr [[TMP22]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_9_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.7 to ptr addrspace(4)), i32 3, ptr [[TMP52]])
// B32-NEXT:    [[TMP56:%.*]] = load target("spirv.Queue"), ptr [[DEFAULT_QUEUE]], align 4
// B32-NEXT:    [[TMP57:%.*]] = load i32, ptr [[FLAGS]], align 4
// B32-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 4 [[TMP24]], ptr align 4 [[NDRANGE]], i32 4, i1 false)
// B32-NEXT:    [[TMP58:%.*]] = getelementptr [1 x i32], ptr [[BLOCK_SIZES25]], i32 0, i32 0
// B32-NEXT:    store i32 0, ptr [[TMP58]], align 4
// B32-NEXT:    [[TMP59:%.*]] = call spir_func i32 @__enqueue_kernel_varargs(target("spirv.Queue") [[TMP56]], i32 [[TMP57]], ptr [[TMP24]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_10_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.8 to ptr addrspace(4)), i32 1, ptr [[TMP58]])
// B32-NEXT:    store ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.9 to ptr addrspace(4)), ptr [[BLOCK_A]], align 4
// B32-NEXT:    store ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.10 to ptr addrspace(4)), ptr [[BLOCK_B]], align 4
// B32-NEXT:    call spir_func void @__device_side_enqueue_block_invoke_11(ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.9 to ptr addrspace(4))) #[[ATTR6:[0-9]+]]
// B32-NEXT:    [[TMP60:%.*]] = load target("spirv.Queue"), ptr [[DEFAULT_QUEUE]], align 4
// B32-NEXT:    [[TMP61:%.*]] = load i32, ptr [[FLAGS]], align 4
// B32-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 4 [[TMP26]], ptr align 4 [[NDRANGE]], i32 4, i1 false)
// B32-NEXT:    [[TMP62:%.*]] = call spir_func i32 @__enqueue_kernel_basic(target("spirv.Queue") [[TMP60]], i32 [[TMP61]], ptr byval([[STRUCT_NDRANGE_T]]) [[TMP26]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_11_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.9 to ptr addrspace(4)))
// B32-NEXT:    [[TMP63:%.*]] = call spir_func i32 @__get_kernel_work_group_size_impl(ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_11_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.9 to ptr addrspace(4)))
// B32-NEXT:    store i32 [[TMP63]], ptr [[SIZE]], align 4
// B32-NEXT:    call spir_func void @__device_side_enqueue_block_invoke_11(ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.9 to ptr addrspace(4))) #[[ATTR6]]
// B32-NEXT:    store ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global to ptr addrspace(4)), ptr [[B1]], align 4
// B32-NEXT:    store ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global to ptr addrspace(4)), ptr [[B2]], align 4
// B32-NEXT:    call spir_func void @block_G_block_invoke(ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global to ptr addrspace(4)), ptr addrspace(3) null) #[[ATTR6]]
// B32-NEXT:    [[TMP64:%.*]] = call spir_func i32 @__get_kernel_preferred_work_group_size_multiple_impl(ptr addrspace(4) addrspacecast (ptr @block_G_block_invoke_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global to ptr addrspace(4)))
// B32-NEXT:    store i32 [[TMP64]], ptr [[SIZE]], align 4
// B32-NEXT:    [[BLOCK_SIZE28:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), i32, ptr addrspace(1) }>, ptr [[BLOCK27]], i32 0, i32 0
// B32-NEXT:    store i32 20, ptr [[BLOCK_SIZE28]], align 4
// B32-NEXT:    [[BLOCK_ALIGN29:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), i32, ptr addrspace(1) }>, ptr [[BLOCK27]], i32 0, i32 1
// B32-NEXT:    store i32 4, ptr [[BLOCK_ALIGN29]], align 4
// B32-NEXT:    [[BLOCK_INVOKE30:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), i32, ptr addrspace(1) }>, ptr [[BLOCK27]], i32 0, i32 2
// B32-NEXT:    store ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_13 to ptr addrspace(4)), ptr [[BLOCK_INVOKE30]], align 4
// B32-NEXT:    [[BLOCK_CAPTURED31:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), i32, ptr addrspace(1) }>, ptr [[BLOCK27]], i32 0, i32 3
// B32-NEXT:    [[TMP65:%.*]] = load i32, ptr [[I_ADDR]], align 4
// B32-NEXT:    store i32 [[TMP65]], ptr [[BLOCK_CAPTURED31]], align 4
// B32-NEXT:    [[BLOCK_CAPTURED32:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), i32, ptr addrspace(1) }>, ptr [[BLOCK27]], i32 0, i32 4
// B32-NEXT:    [[TMP66:%.*]] = load ptr addrspace(1), ptr [[A_ADDR]], align 4
// B32-NEXT:    store ptr addrspace(1) [[TMP66]], ptr [[BLOCK_CAPTURED32]], align 4
// B32-NEXT:    [[BLOCK27_ASCAST:%.*]] = addrspacecast ptr [[BLOCK27]] to ptr addrspace(4)
// B32-NEXT:    store ptr addrspace(4) [[BLOCK27_ASCAST]], ptr [[BLOCK_C]], align 4
// B32-NEXT:    [[TMP67:%.*]] = load target("spirv.Queue"), ptr [[DEFAULT_QUEUE]], align 4
// B32-NEXT:    [[TMP68:%.*]] = load i32, ptr [[FLAGS]], align 4
// B32-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 4 [[TMP33]], ptr align 4 [[NDRANGE]], i32 4, i1 false)
// B32-NEXT:    [[TMP69:%.*]] = load ptr addrspace(4), ptr [[BLOCK_C]], align 4
// B32-NEXT:    [[TMP70:%.*]] = addrspacecast ptr [[BLOCK27]] to ptr addrspace(4)
// B32-NEXT:    [[TMP71:%.*]] = call spir_func i32 @__enqueue_kernel_basic(target("spirv.Queue") [[TMP67]], i32 [[TMP68]], ptr byval([[STRUCT_NDRANGE_T]]) [[TMP33]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_13_kernel to ptr addrspace(4)), ptr addrspace(4) [[TMP70]])
// B32-NEXT:    [[TMP72:%.*]] = call spir_func i32 @__get_kernel_work_group_size_impl(ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_12_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.10 to ptr addrspace(4)))
// B32-NEXT:    store i32 [[TMP72]], ptr [[SIZE]], align 4
// B32-NEXT:    [[TMP73:%.*]] = call spir_func i32 @__get_kernel_preferred_work_group_size_multiple_impl(ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_11_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.9 to ptr addrspace(4)))
// B32-NEXT:    store i32 [[TMP73]], ptr [[SIZE]], align 4
// B32-NEXT:    [[TMP74:%.*]] = call spir_func i32 @__get_kernel_preferred_work_group_size_multiple_impl(ptr addrspace(4) addrspacecast (ptr @block_G_block_invoke_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global to ptr addrspace(4)))
// B32-NEXT:    store i32 [[TMP74]], ptr [[SIZE]], align 4
// B32-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 4 [[TMP34]], ptr align 4 [[NDRANGE]], i32 4, i1 false)
// B32-NEXT:    [[TMP75:%.*]] = call spir_func i32 @__get_kernel_max_sub_group_size_for_ndrange_impl(ptr [[TMP34]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_14_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.11 to ptr addrspace(4)))
// B32-NEXT:    store i32 [[TMP75]], ptr [[SIZE]], align 4
// B32-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 4 [[TMP35]], ptr align 4 [[NDRANGE]], i32 4, i1 false)
// B32-NEXT:    [[TMP76:%.*]] = call spir_func i32 @__get_kernel_sub_group_count_for_ndrange_impl(ptr [[TMP35]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_15_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.12 to ptr addrspace(4)))
// B32-NEXT:    store i32 [[TMP76]], ptr [[SIZE]], align 4
// B32-NEXT:    ret void
//
// X86-LABEL: define dso_local spir_kernel void @device_side_enqueue
// X86-SAME: (ptr addrspace(1) align 4 [[A:%.*]], ptr addrspace(1) align 4 [[B:%.*]], i32 [[I:%.*]]) #[[ATTR3:[0-9]+]] !kernel_arg_addr_space !3 !kernel_arg_access_qual !4 !kernel_arg_type !5 !kernel_arg_base_type !5 !kernel_arg_type_qual !6 {
// X86-NEXT:  entry:
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr addrspace(1), align 8
// X86-NEXT:    [[B_ADDR:%.*]] = alloca ptr addrspace(1), align 8
// X86-NEXT:    [[I_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[DEFAULT_QUEUE:%.*]] = alloca ptr, align 8
// X86-NEXT:    [[FLAGS:%.*]] = alloca i32, align 4
// X86-NEXT:    [[NDRANGE:%.*]] = alloca [[STRUCT_NDRANGE_T:%.*]], align 4
// X86-NEXT:    [[CLK_EVENT:%.*]] = alloca ptr, align 8
// X86-NEXT:    [[EVENT_WAIT_LIST:%.*]] = alloca ptr, align 8
// X86-NEXT:    [[EVENT_WAIT_LIST2:%.*]] = alloca [1 x ptr], align 8
// X86-NEXT:    [[TMP:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// X86-NEXT:    [[BLOCK:%.*]] = alloca <{ i32, i32, ptr addrspace(4), ptr addrspace(1), ptr addrspace(1), i32 }>, align 8
// X86-NEXT:    [[TMP3:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// X86-NEXT:    [[BLOCK4:%.*]] = alloca <{ i32, i32, ptr addrspace(4), ptr addrspace(1), ptr addrspace(1), i32 }>, align 8
// X86-NEXT:    [[TMP11:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// X86-NEXT:    [[TMP12:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// X86-NEXT:    [[BLOCK_SIZES:%.*]] = alloca [1 x i64], align 8
// X86-NEXT:    [[C:%.*]] = alloca i8, align 1
// X86-NEXT:    [[TMP13:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// X86-NEXT:    [[BLOCK_SIZES14:%.*]] = alloca [1 x i64], align 8
// X86-NEXT:    [[TMP15:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// X86-NEXT:    [[BLOCK_SIZES16:%.*]] = alloca [1 x i64], align 8
// X86-NEXT:    [[TMP17:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// X86-NEXT:    [[BLOCK_SIZES19:%.*]] = alloca [1 x i64], align 8
// X86-NEXT:    [[L:%.*]] = alloca i64, align 8
// X86-NEXT:    [[TMP20:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// X86-NEXT:    [[BLOCK_SIZES21:%.*]] = alloca [1 x i64], align 8
// X86-NEXT:    [[TMP22:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// X86-NEXT:    [[BLOCK_SIZES23:%.*]] = alloca [3 x i64], align 8
// X86-NEXT:    [[TMP24:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// X86-NEXT:    [[BLOCK_SIZES25:%.*]] = alloca [1 x i64], align 8
// X86-NEXT:    [[BLOCK_A:%.*]] = alloca ptr addrspace(4), align 8
// X86-NEXT:    [[BLOCK_B:%.*]] = alloca ptr addrspace(4), align 8
// X86-NEXT:    [[TMP26:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// X86-NEXT:    [[SIZE:%.*]] = alloca i32, align 4
// X86-NEXT:    [[B1:%.*]] = alloca ptr addrspace(4), align 8
// X86-NEXT:    [[B2:%.*]] = alloca ptr addrspace(4), align 8
// X86-NEXT:    [[BLOCK_C:%.*]] = alloca ptr addrspace(4), align 8
// X86-NEXT:    [[BLOCK27:%.*]] = alloca <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32 }>, align 8
// X86-NEXT:    [[TMP33:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// X86-NEXT:    [[TMP34:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// X86-NEXT:    [[TMP35:%.*]] = alloca [[STRUCT_NDRANGE_T]], align 4
// X86-NEXT:    store ptr addrspace(1) [[A]], ptr [[A_ADDR]], align 8
// X86-NEXT:    store ptr addrspace(1) [[B]], ptr [[B_ADDR]], align 8
// X86-NEXT:    store i32 [[I]], ptr [[I_ADDR]], align 4
// X86-NEXT:    store i32 0, ptr [[FLAGS]], align 4
// X86-NEXT:    [[ARRAYINIT_BEGIN:%.*]] = getelementptr inbounds [1 x ptr], ptr [[EVENT_WAIT_LIST2]], i64 0, i64 0
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[CLK_EVENT]], align 8
// X86-NEXT:    store ptr [[TMP0]], ptr [[ARRAYINIT_BEGIN]], align 8
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[DEFAULT_QUEUE]], align 8
// X86-NEXT:    [[TMP2:%.*]] = load i32, ptr [[FLAGS]], align 4
// X86-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP]], ptr align 4 [[NDRANGE]], i64 4, i1 false)
// X86-NEXT:    [[BLOCK_SIZE:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), ptr addrspace(1), i32 }>, ptr [[BLOCK]], i32 0, i32 0
// X86-NEXT:    store i32 36, ptr [[BLOCK_SIZE]], align 8
// X86-NEXT:    [[BLOCK_ALIGN:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), ptr addrspace(1), i32 }>, ptr [[BLOCK]], i32 0, i32 1
// X86-NEXT:    store i32 8, ptr [[BLOCK_ALIGN]], align 4
// X86-NEXT:    [[BLOCK_INVOKE:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), ptr addrspace(1), i32 }>, ptr [[BLOCK]], i32 0, i32 2
// X86-NEXT:    store ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke to ptr addrspace(4)), ptr [[BLOCK_INVOKE]], align 8
// X86-NEXT:    [[BLOCK_CAPTURED:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), ptr addrspace(1), i32 }>, ptr [[BLOCK]], i32 0, i32 3
// X86-NEXT:    [[TMP3:%.*]] = load ptr addrspace(1), ptr [[A_ADDR]], align 8
// X86-NEXT:    store ptr addrspace(1) [[TMP3]], ptr [[BLOCK_CAPTURED]], align 8
// X86-NEXT:    [[BLOCK_CAPTURED1:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), ptr addrspace(1), i32 }>, ptr [[BLOCK]], i32 0, i32 5
// X86-NEXT:    [[TMP4:%.*]] = load i32, ptr [[I_ADDR]], align 4
// X86-NEXT:    store i32 [[TMP4]], ptr [[BLOCK_CAPTURED1]], align 8
// X86-NEXT:    [[BLOCK_CAPTURED2:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), ptr addrspace(1), i32 }>, ptr [[BLOCK]], i32 0, i32 4
// X86-NEXT:    [[TMP5:%.*]] = load ptr addrspace(1), ptr [[B_ADDR]], align 8
// X86-NEXT:    store ptr addrspace(1) [[TMP5]], ptr [[BLOCK_CAPTURED2]], align 8
// X86-NEXT:    [[TMP6:%.*]] = addrspacecast ptr [[BLOCK]] to ptr addrspace(4)
// X86-NEXT:    [[TMP7:%.*]] = call i32 @__enqueue_kernel_basic(ptr [[TMP1]], i32 [[TMP2]], ptr byval([[STRUCT_NDRANGE_T]]) [[TMP]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_kernel to ptr addrspace(4)), ptr addrspace(4) [[TMP6]])
// X86-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[DEFAULT_QUEUE]], align 8
// X86-NEXT:    [[TMP9:%.*]] = load i32, ptr [[FLAGS]], align 4
// X86-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP3]], ptr align 4 [[NDRANGE]], i64 4, i1 false)
// X86-NEXT:    [[TMP10:%.*]] = addrspacecast ptr [[EVENT_WAIT_LIST]] to ptr addrspace(4)
// X86-NEXT:    [[TMP11:%.*]] = addrspacecast ptr [[CLK_EVENT]] to ptr addrspace(4)
// X86-NEXT:    [[BLOCK_SIZE5:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), ptr addrspace(1), i32 }>, ptr [[BLOCK4]], i32 0, i32 0
// X86-NEXT:    store i32 36, ptr [[BLOCK_SIZE5]], align 8
// X86-NEXT:    [[BLOCK_ALIGN6:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), ptr addrspace(1), i32 }>, ptr [[BLOCK4]], i32 0, i32 1
// X86-NEXT:    store i32 8, ptr [[BLOCK_ALIGN6]], align 4
// X86-NEXT:    [[BLOCK_INVOKE7:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), ptr addrspace(1), i32 }>, ptr [[BLOCK4]], i32 0, i32 2
// X86-NEXT:    store ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_2 to ptr addrspace(4)), ptr [[BLOCK_INVOKE7]], align 8
// X86-NEXT:    [[BLOCK_CAPTURED8:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), ptr addrspace(1), i32 }>, ptr [[BLOCK4]], i32 0, i32 3
// X86-NEXT:    [[TMP12:%.*]] = load ptr addrspace(1), ptr [[A_ADDR]], align 8
// X86-NEXT:    store ptr addrspace(1) [[TMP12]], ptr [[BLOCK_CAPTURED8]], align 8
// X86-NEXT:    [[BLOCK_CAPTURED9:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), ptr addrspace(1), i32 }>, ptr [[BLOCK4]], i32 0, i32 5
// X86-NEXT:    [[TMP13:%.*]] = load i32, ptr [[I_ADDR]], align 4
// X86-NEXT:    store i32 [[TMP13]], ptr [[BLOCK_CAPTURED9]], align 8
// X86-NEXT:    [[BLOCK_CAPTURED10:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), ptr addrspace(1), i32 }>, ptr [[BLOCK4]], i32 0, i32 4
// X86-NEXT:    [[TMP14:%.*]] = load ptr addrspace(1), ptr [[B_ADDR]], align 8
// X86-NEXT:    store ptr addrspace(1) [[TMP14]], ptr [[BLOCK_CAPTURED10]], align 8
// X86-NEXT:    [[TMP15:%.*]] = addrspacecast ptr [[BLOCK4]] to ptr addrspace(4)
// X86-NEXT:    [[TMP16:%.*]] = call i32 @__enqueue_kernel_basic_events(ptr [[TMP8]], i32 [[TMP9]], ptr [[TMP3]], i32 2, ptr addrspace(4) [[TMP10]], ptr addrspace(4) [[TMP11]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_2_kernel to ptr addrspace(4)), ptr addrspace(4) [[TMP15]])
// X86-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[DEFAULT_QUEUE]], align 8
// X86-NEXT:    [[TMP18:%.*]] = load i32, ptr [[FLAGS]], align 4
// X86-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP11]], ptr align 4 [[NDRANGE]], i64 4, i1 false)
// X86-NEXT:    [[TMP19:%.*]] = call i32 @__enqueue_kernel_basic_events(ptr [[TMP17]], i32 [[TMP18]], ptr [[TMP11]], i32 1, ptr addrspace(4) null, ptr addrspace(4) null, ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_3_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.1 to ptr addrspace(4)))
// X86-NEXT:    [[TMP20:%.*]] = load ptr, ptr [[DEFAULT_QUEUE]], align 8
// X86-NEXT:    [[TMP21:%.*]] = load i32, ptr [[FLAGS]], align 4
// X86-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP12]], ptr align 4 [[NDRANGE]], i64 4, i1 false)
// X86-NEXT:    [[TMP22:%.*]] = getelementptr [1 x i64], ptr [[BLOCK_SIZES]], i32 0, i32 0
// X86-NEXT:    store i64 256, ptr [[TMP22]], align 8
// X86-NEXT:    [[TMP23:%.*]] = call i32 @__enqueue_kernel_varargs(ptr [[TMP20]], i32 [[TMP21]], ptr [[TMP12]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_4_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.2 to ptr addrspace(4)), i32 1, ptr [[TMP22]])
// X86-NEXT:    [[TMP24:%.*]] = load ptr, ptr [[DEFAULT_QUEUE]], align 8
// X86-NEXT:    [[TMP25:%.*]] = load i32, ptr [[FLAGS]], align 4
// X86-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP13]], ptr align 4 [[NDRANGE]], i64 4, i1 false)
// X86-NEXT:    [[TMP26:%.*]] = getelementptr [1 x i64], ptr [[BLOCK_SIZES14]], i32 0, i32 0
// X86-NEXT:    [[TMP27:%.*]] = load i8, ptr [[C]], align 1
// X86-NEXT:    [[TMP28:%.*]] = zext i8 [[TMP27]] to i64
// X86-NEXT:    store i64 [[TMP28]], ptr [[TMP26]], align 8
// X86-NEXT:    [[TMP29:%.*]] = call i32 @__enqueue_kernel_varargs(ptr [[TMP24]], i32 [[TMP25]], ptr [[TMP13]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_5_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.3 to ptr addrspace(4)), i32 1, ptr [[TMP26]])
// X86-NEXT:    [[TMP30:%.*]] = load ptr, ptr [[DEFAULT_QUEUE]], align 8
// X86-NEXT:    [[TMP31:%.*]] = load i32, ptr [[FLAGS]], align 4
// X86-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP15]], ptr align 4 [[NDRANGE]], i64 4, i1 false)
// X86-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [1 x ptr], ptr [[EVENT_WAIT_LIST2]], i64 0, i64 0
// X86-NEXT:    [[TMP32:%.*]] = addrspacecast ptr [[ARRAYDECAY]] to ptr addrspace(4)
// X86-NEXT:    [[TMP33:%.*]] = addrspacecast ptr [[CLK_EVENT]] to ptr addrspace(4)
// X86-NEXT:    [[TMP34:%.*]] = getelementptr [1 x i64], ptr [[BLOCK_SIZES16]], i32 0, i32 0
// X86-NEXT:    store i64 256, ptr [[TMP34]], align 8
// X86-NEXT:    [[TMP35:%.*]] = call i32 @__enqueue_kernel_events_varargs(ptr [[TMP30]], i32 [[TMP31]], ptr [[TMP15]], i32 2, ptr addrspace(4) [[TMP32]], ptr addrspace(4) [[TMP33]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_6_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.4 to ptr addrspace(4)), i32 1, ptr [[TMP34]])
// X86-NEXT:    [[TMP36:%.*]] = load ptr, ptr [[DEFAULT_QUEUE]], align 8
// X86-NEXT:    [[TMP37:%.*]] = load i32, ptr [[FLAGS]], align 4
// X86-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP17]], ptr align 4 [[NDRANGE]], i64 4, i1 false)
// X86-NEXT:    [[ARRAYDECAY18:%.*]] = getelementptr inbounds [1 x ptr], ptr [[EVENT_WAIT_LIST2]], i64 0, i64 0
// X86-NEXT:    [[TMP38:%.*]] = addrspacecast ptr [[ARRAYDECAY18]] to ptr addrspace(4)
// X86-NEXT:    [[TMP39:%.*]] = addrspacecast ptr [[CLK_EVENT]] to ptr addrspace(4)
// X86-NEXT:    [[TMP40:%.*]] = getelementptr [1 x i64], ptr [[BLOCK_SIZES19]], i32 0, i32 0
// X86-NEXT:    [[TMP41:%.*]] = load i8, ptr [[C]], align 1
// X86-NEXT:    [[TMP42:%.*]] = zext i8 [[TMP41]] to i64
// X86-NEXT:    store i64 [[TMP42]], ptr [[TMP40]], align 8
// X86-NEXT:    [[TMP43:%.*]] = call i32 @__enqueue_kernel_events_varargs(ptr [[TMP36]], i32 [[TMP37]], ptr [[TMP17]], i32 2, ptr addrspace(4) [[TMP38]], ptr addrspace(4) [[TMP39]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_7_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.5 to ptr addrspace(4)), i32 1, ptr [[TMP40]])
// X86-NEXT:    [[TMP44:%.*]] = load ptr, ptr [[DEFAULT_QUEUE]], align 8
// X86-NEXT:    [[TMP45:%.*]] = load i32, ptr [[FLAGS]], align 4
// X86-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP20]], ptr align 4 [[NDRANGE]], i64 4, i1 false)
// X86-NEXT:    [[TMP46:%.*]] = getelementptr [1 x i64], ptr [[BLOCK_SIZES21]], i32 0, i32 0
// X86-NEXT:    [[TMP47:%.*]] = load i64, ptr [[L]], align 8
// X86-NEXT:    store i64 [[TMP47]], ptr [[TMP46]], align 8
// X86-NEXT:    [[TMP48:%.*]] = call i32 @__enqueue_kernel_varargs(ptr [[TMP44]], i32 [[TMP45]], ptr [[TMP20]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_8_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.6 to ptr addrspace(4)), i32 1, ptr [[TMP46]])
// X86-NEXT:    [[TMP49:%.*]] = load ptr, ptr [[DEFAULT_QUEUE]], align 8
// X86-NEXT:    [[TMP50:%.*]] = load i32, ptr [[FLAGS]], align 4
// X86-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP22]], ptr align 4 [[NDRANGE]], i64 4, i1 false)
// X86-NEXT:    [[TMP51:%.*]] = getelementptr [3 x i64], ptr [[BLOCK_SIZES23]], i32 0, i32 0
// X86-NEXT:    store i64 1, ptr [[TMP51]], align 8
// X86-NEXT:    [[TMP52:%.*]] = getelementptr [3 x i64], ptr [[BLOCK_SIZES23]], i32 0, i32 1
// X86-NEXT:    store i64 2, ptr [[TMP52]], align 8
// X86-NEXT:    [[TMP53:%.*]] = getelementptr [3 x i64], ptr [[BLOCK_SIZES23]], i32 0, i32 2
// X86-NEXT:    store i64 4, ptr [[TMP53]], align 8
// X86-NEXT:    [[TMP54:%.*]] = call i32 @__enqueue_kernel_varargs(ptr [[TMP49]], i32 [[TMP50]], ptr [[TMP22]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_9_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.7 to ptr addrspace(4)), i32 3, ptr [[TMP51]])
// X86-NEXT:    [[TMP55:%.*]] = load ptr, ptr [[DEFAULT_QUEUE]], align 8
// X86-NEXT:    [[TMP56:%.*]] = load i32, ptr [[FLAGS]], align 4
// X86-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP24]], ptr align 4 [[NDRANGE]], i64 4, i1 false)
// X86-NEXT:    [[TMP57:%.*]] = getelementptr [1 x i64], ptr [[BLOCK_SIZES25]], i32 0, i32 0
// X86-NEXT:    store i64 4294967296, ptr [[TMP57]], align 8
// X86-NEXT:    [[TMP58:%.*]] = call i32 @__enqueue_kernel_varargs(ptr [[TMP55]], i32 [[TMP56]], ptr [[TMP24]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_10_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.8 to ptr addrspace(4)), i32 1, ptr [[TMP57]])
// X86-NEXT:    store ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.9 to ptr addrspace(4)), ptr [[BLOCK_A]], align 8
// X86-NEXT:    store ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.10 to ptr addrspace(4)), ptr [[BLOCK_B]], align 8
// X86-NEXT:    call void @__device_side_enqueue_block_invoke_11(ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.9 to ptr addrspace(4))) #[[ATTR6:[0-9]+]]
// X86-NEXT:    [[TMP59:%.*]] = load ptr, ptr [[DEFAULT_QUEUE]], align 8
// X86-NEXT:    [[TMP60:%.*]] = load i32, ptr [[FLAGS]], align 4
// X86-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP26]], ptr align 4 [[NDRANGE]], i64 4, i1 false)
// X86-NEXT:    [[TMP61:%.*]] = call i32 @__enqueue_kernel_basic(ptr [[TMP59]], i32 [[TMP60]], ptr byval([[STRUCT_NDRANGE_T]]) [[TMP26]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_11_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.9 to ptr addrspace(4)))
// X86-NEXT:    [[TMP62:%.*]] = call i32 @__get_kernel_work_group_size_impl(ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_11_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.9 to ptr addrspace(4)))
// X86-NEXT:    store i32 [[TMP62]], ptr [[SIZE]], align 4
// X86-NEXT:    call void @__device_side_enqueue_block_invoke_11(ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.9 to ptr addrspace(4))) #[[ATTR6]]
// X86-NEXT:    store ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global to ptr addrspace(4)), ptr [[B1]], align 8
// X86-NEXT:    store ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global to ptr addrspace(4)), ptr [[B2]], align 8
// X86-NEXT:    call void @block_G_block_invoke(ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global to ptr addrspace(4)), ptr addrspace(3) null) #[[ATTR6]]
// X86-NEXT:    [[TMP63:%.*]] = call i32 @__get_kernel_preferred_work_group_size_multiple_impl(ptr addrspace(4) addrspacecast (ptr @block_G_block_invoke_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global to ptr addrspace(4)))
// X86-NEXT:    store i32 [[TMP63]], ptr [[SIZE]], align 4
// X86-NEXT:    [[BLOCK_SIZE28:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32 }>, ptr [[BLOCK27]], i32 0, i32 0
// X86-NEXT:    store i32 28, ptr [[BLOCK_SIZE28]], align 8
// X86-NEXT:    [[BLOCK_ALIGN29:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32 }>, ptr [[BLOCK27]], i32 0, i32 1
// X86-NEXT:    store i32 8, ptr [[BLOCK_ALIGN29]], align 4
// X86-NEXT:    [[BLOCK_INVOKE30:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32 }>, ptr [[BLOCK27]], i32 0, i32 2
// X86-NEXT:    store ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_13 to ptr addrspace(4)), ptr [[BLOCK_INVOKE30]], align 8
// X86-NEXT:    [[BLOCK_CAPTURED31:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32 }>, ptr [[BLOCK27]], i32 0, i32 4
// X86-NEXT:    [[TMP64:%.*]] = load i32, ptr [[I_ADDR]], align 4
// X86-NEXT:    store i32 [[TMP64]], ptr [[BLOCK_CAPTURED31]], align 8
// X86-NEXT:    [[BLOCK_CAPTURED32:%.*]] = getelementptr inbounds <{ i32, i32, ptr addrspace(4), ptr addrspace(1), i32 }>, ptr [[BLOCK27]], i32 0, i32 3
// X86-NEXT:    [[TMP65:%.*]] = load ptr addrspace(1), ptr [[A_ADDR]], align 8
// X86-NEXT:    store ptr addrspace(1) [[TMP65]], ptr [[BLOCK_CAPTURED32]], align 8
// X86-NEXT:    [[BLOCK27_ASCAST:%.*]] = addrspacecast ptr [[BLOCK27]] to ptr addrspace(4)
// X86-NEXT:    store ptr addrspace(4) [[BLOCK27_ASCAST]], ptr [[BLOCK_C]], align 8
// X86-NEXT:    [[TMP66:%.*]] = load ptr, ptr [[DEFAULT_QUEUE]], align 8
// X86-NEXT:    [[TMP67:%.*]] = load i32, ptr [[FLAGS]], align 4
// X86-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP33]], ptr align 4 [[NDRANGE]], i64 4, i1 false)
// X86-NEXT:    [[TMP68:%.*]] = load ptr addrspace(4), ptr [[BLOCK_C]], align 8
// X86-NEXT:    [[TMP69:%.*]] = addrspacecast ptr [[BLOCK27]] to ptr addrspace(4)
// X86-NEXT:    [[TMP70:%.*]] = call i32 @__enqueue_kernel_basic(ptr [[TMP66]], i32 [[TMP67]], ptr byval([[STRUCT_NDRANGE_T]]) [[TMP33]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_13_kernel to ptr addrspace(4)), ptr addrspace(4) [[TMP69]])
// X86-NEXT:    [[TMP71:%.*]] = call i32 @__get_kernel_work_group_size_impl(ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_12_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.10 to ptr addrspace(4)))
// X86-NEXT:    store i32 [[TMP71]], ptr [[SIZE]], align 4
// X86-NEXT:    [[TMP72:%.*]] = call i32 @__get_kernel_preferred_work_group_size_multiple_impl(ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_11_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.9 to ptr addrspace(4)))
// X86-NEXT:    store i32 [[TMP72]], ptr [[SIZE]], align 4
// X86-NEXT:    [[TMP73:%.*]] = call i32 @__get_kernel_preferred_work_group_size_multiple_impl(ptr addrspace(4) addrspacecast (ptr @block_G_block_invoke_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global to ptr addrspace(4)))
// X86-NEXT:    store i32 [[TMP73]], ptr [[SIZE]], align 4
// X86-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP34]], ptr align 4 [[NDRANGE]], i64 4, i1 false)
// X86-NEXT:    [[TMP74:%.*]] = call i32 @__get_kernel_max_sub_group_size_for_ndrange_impl(ptr [[TMP34]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_14_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.11 to ptr addrspace(4)))
// X86-NEXT:    store i32 [[TMP74]], ptr [[SIZE]], align 4
// X86-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP35]], ptr align 4 [[NDRANGE]], i64 4, i1 false)
// X86-NEXT:    [[TMP75:%.*]] = call i32 @__get_kernel_sub_group_count_for_ndrange_impl(ptr [[TMP35]], ptr addrspace(4) addrspacecast (ptr @__device_side_enqueue_block_invoke_15_kernel to ptr addrspace(4)), ptr addrspace(4) addrspacecast (ptr addrspace(1) @__block_literal_global.12 to ptr addrspace(4)))
// X86-NEXT:    store i32 [[TMP75]], ptr [[SIZE]], align 4
// X86-NEXT:    ret void
//
kernel void device_side_enqueue(global int *a, global int *b, int i) {
  queue_t default_queue;
  unsigned flags = 0;
  ndrange_t ndrange;
  clk_event_t clk_event;
  clk_event_t event_wait_list;
  clk_event_t event_wait_list2[] = {clk_event};



  // Emits block literal on stack and block kernel [[INVLK1]].
  enqueue_kernel(default_queue, flags, ndrange,
                 ^(void) {
                   a[i] = b[i];
                 });

  // Emits block literal on stack and block kernel [[INVLK2]].
  enqueue_kernel(default_queue, flags, ndrange, 2, &event_wait_list, &clk_event,
                 ^(void) {
                   a[i] = b[i];
                 });

  enqueue_kernel(default_queue, flags, ndrange, 1, 0, 0,
                 ^(void) {
                   return;
                 });

  // Emits global block literal [[BLG1]] and block kernel [[INVGK1]].
  enqueue_kernel(default_queue, flags, ndrange,
                 ^(local void *p) {
                   return;
                 },
                 256);

  char c;
  // Emits global block literal [[BLG2]] and block kernel [[INVGK2]].
  enqueue_kernel(default_queue, flags, ndrange,
                 ^(local void *p) {
                   return;
                 },
                 c);

  // Emits global block literal [[BLG3]] and block kernel [[INVGK3]].
  enqueue_kernel(default_queue, flags, ndrange, 2, event_wait_list2, &clk_event,
                 ^(local void *p) {
                   return;
                 },
                 256);

  // Emits global block literal [[BLG4]] and block kernel [[INVGK4]].
  enqueue_kernel(default_queue, flags, ndrange, 2, event_wait_list2, &clk_event,
                 ^(local void *p) {
                   return;
                 },
                 c);

  long l;
  // Emits global block literal [[BLG5]] and block kernel [[INVGK5]].
  enqueue_kernel(default_queue, flags, ndrange,
                 ^(local void *p) {
                   return;
                 },
                 l);

  // Emits global block literal [[BLG6]] and block kernel [[INVGK6]].
  enqueue_kernel(default_queue, flags, ndrange,
                 ^(local void *p1, local void *p2, local void *p3) {
                   return;
                 },
                 1, 2, 4);

  // Emits global block literal [[BLG7]] and block kernel [[INVGK7]].
  enqueue_kernel(default_queue, flags, ndrange,
                 ^(local void *p) {
                   return;
                 },
                 4294967296L);

  // Emits global block literal [[BLG8]] and invoke function [[INVG8]].
  // The full type of these expressions are long (and repeated elsewhere), so we
  // capture it as part of the regex for convenience and clarity.
  void (^const block_A)(void) = ^{
    return;
  };

  // Emits global block literal [[BLG9]] and invoke function [[INVG9]].
  void (^const block_B)(local void *) = ^(local void *a) {
    return;
  };

  // Uses global block literal [[BLG8]] and invoke function [[INVG8]].
  block_A();

  // Emits global block literal [[BLG8]] and block kernel [[INVGK8]]. [[INVGK8]] calls [[INVG8]].
  enqueue_kernel(default_queue, flags, ndrange, block_A);

  // Uses block kernel [[INVGK8]] and global block literal [[BLG8]].
  unsigned size = get_kernel_work_group_size(block_A);

  // Uses global block literal [[BLG8]] and invoke function [[INVG8]]. Make sure no redundant block literal and invoke functions are emitted.
  block_A();

  // Make sure that block invoke function is resolved correctly after sequence of assignements.
  bl_t b1 = block_G;
  bl_t b2 = b1;
  // COOMON-SAME: to ptr addrspace(4)), ptr addrspace(3) null)
  b2(0);
  // Uses global block literal [[BL_GLOBAL]] and block kernel [[INV_G_K]]. [[INV_G_K]] calls [[INV_G]].
  size = get_kernel_preferred_work_group_size_multiple(b2);

  void (^block_C)(void) = ^{
    callee(i, a);
  };
  // Emits block literal on stack and block kernel [[INVLK3]].
  enqueue_kernel(default_queue, flags, ndrange, block_C);

  // Emits global block literal [[BLG9]] and block kernel [[INVGK9]]. [[INVGK9]] calls [[INV9]].
  size = get_kernel_work_group_size(block_B);

  // Uses global block literal [[BLG8]] and block kernel [[INVGK8]]. Make sure no redundant block literal ind invoke functions are emitted.
  size = get_kernel_preferred_work_group_size_multiple(block_A);

  // Uses global block literal [[BL_GLOBAL]] and block kernel [[INV_G_K]]. [[INV_G_K]] calls [[INV_G]].
  size = get_kernel_preferred_work_group_size_multiple(block_G);

  // Emits global block literal [[BLG10]] and block kernel [[INVGK10]].
  size = get_kernel_max_sub_group_size_for_ndrange(ndrange, ^(){});

  // Emits global block literal [[BLG11]] and block kernel [[INVGK11]].
  size = get_kernel_sub_group_count_for_ndrange(ndrange, ^(){});
}


//// NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
// B64: {{.*}}
// CHECK-LIFETIMES: {{.*}}
// COMMON: {{.*}}
// SPIR: {{.*}}
